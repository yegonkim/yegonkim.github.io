---
title: Does Long-Horizon RL Really Provide 1-Bit of Information Per Episode?
categories: Artificial Intelligence
date: 2025-10-29
math: true
published: false
---

what would the future look like in 2028
we don't know.
but we can make assumptions about ai capabilities,
and think of what the optimal research process would look like given those capabilities.
if we still have humans in the loop, what would their role be?
humans can have deep understanding of things with small data. perhaps ai will learn to leverage that. they will learn what form of reports can best elicit useful human help for getting better performance on their benchmarks. 


most intuition comes from experience

i find arc agi scary, especially 3. what will happen when we have ai systems that can do that level of reasoning and planning?


[AlphaEvolve](https://arxiv.org/abs/2506.13131) made me think more seriously about the possibility that AI systems automate research. In particular, it points at a future where AI can optimize against cheaply runnable benchmarks 

If there are benchmarks and optimization on those benchmarks, what's the role of "understanding"? If AIs truly are superhuman at optimizing benchmarks, then human understanding will 

Qs:
how clear is the line between benchmarks that evaluate and benchmarks that innovate?


academia is highly inefficient. mostly due to communication and incentive problems.


i used to think improving capability A by using capability A is not possible, that it would violate some sort of no-free-lunch theorem. but this might not be the case. if A depends on subcapabilities B and C, then A could be used to improve B and C, which in turn improves A.

https://arxiv.org/abs/1803.03453

analyze actual outputs of alphaevolve.
honestly the outputs don't really impress me. but the results do. it seems like adding the half integer regularization was due to the prompt, and the fact that it works well. to be clear, the prompt didn't specify how to achieve this, so that's impressive. but i don't see anything not mentioned in the prompt. but this is understandable since the problem is probably quite hard, and a very very large innovation is probably needed to make an improvement.
how creative it actually is now, and how better it will probably get.

this gives me hope that scaling ai researchers will also scalably solve the alignment problem.

visual recognition -> cifar10
algorithm learning and execution (learning with/without execution trace)-> math competitions 
long term memory
compositional generalization -> scan
length generalization -> parity
perfect information two player zero sum game -> Go
sample efficiency -> arc
exploration -> Montezuma's Revenge
planning -> 
image understanding -> 

long-term credit assignment

i think some of these mistaken for evaluating other capabilities, e.g. maths competition for general reasoning.


in general we can use eval for two purposes:
- evaluating the progress of current AI in what the eval is trying to measure (e.g. gdpval)
- optimizing the AI system against the eval, to make progress

due to goodhart's law, we probably shouldn't be using the same eval for both purposes. fortunately, 

but overfitting to synthetic tasks can be problematic. for the final optimization we would probably need to optimize against realistic tasks, by using novel ideas proposed during optimization against the extreme cases in synthetic tasks.

another thing to note is that many of the historical "solutions" to these benchmarks involved using existing ideas and composing them, with some slight tweaks here and there. the usefulness of benchmarks is not only in eliciting new ideas, but also in picking out the ideas that actually work when tweaked ans scaled the right way.

this conforms to the bitter lesson. we should scale up the search for solution, whether it's sgd or evolutionary search of architectures, or even the training data. and of course, we should scale the learning of the system that does the search.

the most valuable researchers will be those who have a good grasp of what's missing in the current AI systems, and propose relevant tasks that actually require novel ideas to solve. 


can we "meta"-optimize the agent that performs the optimization against these benchmarks?
it seems quite clear to me that certain general abilties, like long term memory, sample efficiency, world modeling, and planning, will lead to direct improvements in this "meta"-skill. that's why it's the most productive to think of abilities that are so general that optimizing for it leads to improvements in the ability to optimize itself.
for instance, according to ablation, alphaevolve benefits a lot from good initial idea. retrieving the good initial ideas from the large pile of existing ideas would be a helpful skill.

how can llms organize their research? academic papers might not be the optimal choice. we might have some sort of "research logs" that are more structured and easier to parse.


I find it hard to believe that evolution had high selective pressure for extremely long horizon tasks, such as planning for months. 


how is it better than human researchers?
- can run in parallel (many ais)
- clear reproducibility and hence better credit assignment
- faster iteration (time to code, run scripts, job scheduling, etc.)
- better structured knowledge of other agents' research and hence less time spent on reading papers
- more effective collaboration (e.g. through shared codebases, tools, and environments)
- no forgetting over time


benchmarks that evaluate ai progress will require a lot of resources, possibly a community collaboration.

researchers will become benchmark creators, and the best researchers will be those who can create the most challenging benchmarks that require novel ideas to solve. and the skill of optimizing against these benchmarks will be less important â€” any ai will be able to do that. this doesn't mean that insight on optimization is no longer needed. i believe that humans will still be better than ai at deep insights about why certain ideas work, for the forseeable future. (based on extrapolation of past ais abilities to build )
i.e. people who actually know what they're doing.

i think this might be somewhat hindered by centralization of ai resources to researchers who were good at optimizing.

in general, solving a problem will involve humans who are good at specifying the problem in such a manner that a solution optimized by an ai will actually solve the problem in the real world. this is a skill in itself that will be very valuable for a long time, as it requires deep understanding of the problem domain, and an intuition about the capabilities of the ai systems.
traditionally, the role of benchmark creation was done by a few humans, 
there are many problems in the world, so many that there will never be a shortage of them. with the advent of optimizing ai systems, the bottleneck will shift to the quality and quantity of humans who can specify the problems in a way that ai systems can solve. it's funny, because creating benchmarks is often seen as a menial task, and somewhat looked upon by some immature researchers.
and make no mistake, this doesn't mean that academic papers will just become proposals for benchmarks. that would just be slop. valuable papers will be those that propose benchmarks to optimize against, and actually show that optimizing against them can produce solutions that are favorable in terms of the test benchmarks.
however, a concern is that the researchers will use the ai optimizers to optimize against the test benchmarks (which they most do already although through human optimization), 


we now should think not of "in which environments will gradient descent lead to our desired capabilities?", but "which environments/fields/problems will lead to the discovery of our desired capabilities when optimized against by human/ai researchers?"