---
title: 1-bit RL
categories: Artificial Intelligence
date: 2025-11-01
math: true
published: true
---

A [recent writing by Toby Ord](https://www.tobyord.com/writing/inefficiency-of-reinforcement-learning) claims that RLVR provides at most 1 bit of information per episode, arguing that this severely limits the efficiency of the current RLVR paradigm. This claim has sparked some [discussions on X](https://x.com/khoomeik/status/1979236183717875944), some claiming that this 1-bit limit is not much of a problem because the 1-bit is highly focused on the information that we want to know. For example, if we are training an RL agent to play chess, the 1-bit of information tells us whether the agent won or lost the game, which is very relevant to our goal of training a strong chess player.

https://www.tobyord.com/writing/inefficiency-of-reinforcement-learning

https://thinkingmachines.ai/blog/lora/




Does training reward model with human feedback and training RL agent with the reward model provide additional info? No! this is the most common misconception I see. you can never get more information without interacting with the environment. You *can* get V-information since 

